{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isamdr86/ai-project/blob/main/notebooks/Web_Search_API_ir.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JboB5VaCJUrb"
      },
      "outputs": [],
      "source": [
        "!pip install -q llama-index==0.10.57 openai==1.37.0 tiktoken==0.7.0 llama-index-tools-google==0.1.3 newspaper4k==0.9.3.1 lxml_html_clean"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install openai==1.55.3 httpx==0.27.2 --force-reinstall --quiet"
      ],
      "metadata": {
        "id": "_IxPPFVWX6vN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "eXvuUIalYeKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1NKAn5scN_g9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_api_key')\n",
        "\n",
        "GOOGLE_SEARCH_KEY = userdata.get('google_search_api_key')\n",
        "GOOGLE_SEARCH_ENGINE = userdata.get('google_engine_id') # Search Engine ID"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM and Embedding Model"
      ],
      "metadata": {
        "id": "Yp-tXudCc3Zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature= 0)\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
      ],
      "metadata": {
        "id": "dYmz-uAIc2rb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex1gQVHvITMI"
      },
      "source": [
        "# Using Agents/Tools\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import FunctionTool\n",
        "from llama_index.core.agent import ReActAgent\n",
        "\n",
        "\n",
        "# define sample Tool\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply two integers and returns the result integer\"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
        "\n",
        "# initialize ReAct agent\n",
        "agent = ReActAgent.from_tools([multiply_tool], verbose=True)"
      ],
      "metadata": {
        "id": "OHd8WbUoZHWi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = agent.chat(\"What is the multiplication of 43 and 45?\")\n",
        "\n",
        "# Final response\n",
        "res.response"
      ],
      "metadata": {
        "id": "9JGVdS6AZGve",
        "outputId": "d704a691-ef84-4ca4-cdf7-d72d4ae78976",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Running step 2a90f31f-9dfc-4847-93c8-f656c435c5c8. Step input: What is the multiplication of 43 and 45?\n",
            "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
            "Action: multiply\n",
            "Action Input: {'a': 43, 'b': 45}\n",
            "\u001b[0m\u001b[1;3;34mObservation: 1935\n",
            "\u001b[0m> Running step 0087f48b-c3ab-4b22-b0ca-5f7d67e0ff1c. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n",
            "Answer: The multiplication of 43 and 45 is 1935.\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The multiplication of 43 and 45 is 1935.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LMypoqUyuXq"
      },
      "source": [
        "## Define Google Search Tool\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4Q7sc69nJvWI"
      },
      "outputs": [],
      "source": [
        "from llama_index.tools.google import GoogleSearchToolSpec\n",
        "\n",
        "tool_spec = GoogleSearchToolSpec(key=GOOGLE_SEARCH_KEY, engine=GOOGLE_SEARCH_ENGINE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VrbuIOaMeOIf"
      },
      "outputs": [],
      "source": [
        "# Import and initialize our tool spec\n",
        "from llama_index.core.tools.tool_spec.load_and_search import LoadAndSearchToolSpec\n",
        "\n",
        "# Wrap the google search tool to create an index on top of the returned Google search\n",
        "wrapped_tool = LoadAndSearchToolSpec.from_defaults(\n",
        "    tool_spec.to_tool_list()[0],\n",
        ").to_tool_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3ENpLyBy7UL"
      },
      "source": [
        "## Create the Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-_Ab47ppK8b2"
      },
      "outputs": [],
      "source": [
        "from llama_index.agent.openai import OpenAIAgent\n",
        "\n",
        "agent = OpenAIAgent.from_tools(wrapped_tool, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YcUyz1-FlCQ8"
      },
      "outputs": [],
      "source": [
        "res = agent.chat(\"How many parameters LLaMA 3.2 model has?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "w4wK5sY-lOOv",
        "outputId": "ea6fea1b-cf8d-4b00-dad9-2a99e550a645"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The LLaMA 3.2 model has the following parameters:\\n\\n- Vision models: 11 billion and 90 billion parameters\\n- Text-only models: 1 billion and 3 billion parameters'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "res.response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TM_cvBA1nTJM",
        "outputId": "9c1ec16c-96a5-4633-bf44-380726c36adc",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[ToolOutput(content='Content loaded! You can now search the information using read_google_search', tool_name='google_search', raw_input={'args': (), 'kwargs': {'query': 'LLaMA 3.2 model parameters'}}, raw_output='Content loaded! You can now search the information using read_google_search', is_error=False),\n",
              " ToolOutput(content='Llama 3.2 features small and medium-sized vision models with 11 billion and 90 billion parameters, alongside lightweight text-only models with 1 billion and 3 billion parameters. It also introduces the Llama Stack Distribution.', tool_name='read_google_search', raw_input={'args': (), 'kwargs': {'query': 'LLaMA 3.2 model parameters'}}, raw_output='Llama 3.2 features small and medium-sized vision models with 11 billion and 90 billion parameters, alongside lightweight text-only models with 1 billion and 3 billion parameters. It also introduces the Llama Stack Distribution.', is_error=False)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "res.sources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "who-NM4pIhPn"
      },
      "source": [
        "# Using Tools w/ VectorStoreIndex\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9g9cTM9GI-19"
      },
      "source": [
        "A limitation of the current agent/tool in LlamaIndex is that it **relies solely on the page description from the retrieved pages** to answer questions. This approach will miss answers that are not visible in the page's description tag. To address this, a possible workaround is to fetch the page results, extract the page content using the newspaper3k library, and then create an index based on the downloaded content. Also, the previous method stacks all retrieved items from the search engine into a single document, making it **difficult to pinpoint the exact source** of the response. However, the following method will enable us to present the sources easily.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31G_fxxJIsbC"
      },
      "source": [
        "## Define Google Search Tool\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lwRmj2odIHxt"
      },
      "outputs": [],
      "source": [
        "from llama_index.tools.google import GoogleSearchToolSpec\n",
        "\n",
        "tool_spec = GoogleSearchToolSpec(key=GOOGLE_SEARCH_KEY, engine=GOOGLE_SEARCH_ENGINE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UVIxdj04Bsf2"
      },
      "outputs": [],
      "source": [
        "search_results = tool_spec.google_search(\"LLaMA 3.2 model details\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "AlYDNfg2BsdQ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "search_results = json.loads(search_results[0].text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search_results"
      ],
      "metadata": {
        "id": "bx_V-aNfXgR8",
        "outputId": "09e71c7a-b928-4841-931f-2d227e7f13f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kind': 'customsearch#search',\n",
              " 'url': {'type': 'application/json',\n",
              "  'template': 'https://www.googleapis.com/customsearch/v1?q={searchTerms}&num={count?}&start={startIndex?}&lr={language?}&safe={safe?}&cx={cx?}&sort={sort?}&filter={filter?}&gl={gl?}&cr={cr?}&googlehost={googleHost?}&c2coff={disableCnTwTranslation?}&hq={hq?}&hl={hl?}&siteSearch={siteSearch?}&siteSearchFilter={siteSearchFilter?}&exactTerms={exactTerms?}&excludeTerms={excludeTerms?}&linkSite={linkSite?}&orTerms={orTerms?}&dateRestrict={dateRestrict?}&lowRange={lowRange?}&highRange={highRange?}&searchType={searchType}&fileType={fileType?}&rights={rights?}&imgSize={imgSize?}&imgType={imgType?}&imgColorType={imgColorType?}&imgDominantColor={imgDominantColor?}&alt=json'},\n",
              " 'queries': {'request': [{'title': 'Google Custom Search - LLaMA 3.2 model details',\n",
              "    'totalResults': '6620000',\n",
              "    'searchTerms': 'LLaMA 3.2 model details',\n",
              "    'count': 10,\n",
              "    'startIndex': 1,\n",
              "    'inputEncoding': 'utf8',\n",
              "    'outputEncoding': 'utf8',\n",
              "    'safe': 'off',\n",
              "    'cx': 'a5101842112354c68'}],\n",
              "  'nextPage': [{'title': 'Google Custom Search - LLaMA 3.2 model details',\n",
              "    'totalResults': '6620000',\n",
              "    'searchTerms': 'LLaMA 3.2 model details',\n",
              "    'count': 10,\n",
              "    'startIndex': 11,\n",
              "    'inputEncoding': 'utf8',\n",
              "    'outputEncoding': 'utf8',\n",
              "    'safe': 'off',\n",
              "    'cx': 'a5101842112354c68'}]},\n",
              " 'context': {'title': 'towardsai'},\n",
              " 'searchInformation': {'searchTime': 0.18972,\n",
              "  'formattedSearchTime': '0.19',\n",
              "  'totalResults': '6620000',\n",
              "  'formattedTotalResults': '6,620,000'},\n",
              " 'items': [{'kind': 'customsearch#result',\n",
              "   'title': 'Llama 3.2: Revolutionizing edge AI and vision with open ...',\n",
              "   'htmlTitle': '<b>Llama 3.2</b>: Revolutionizing edge AI and vision with open ...',\n",
              "   'link': 'https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/',\n",
              "   'displayLink': 'ai.meta.com',\n",
              "   'snippet': 'Sep 25, 2024 ... The Llama 3.2 1B and 3B models support context length of 128K tokens and are state-of-the-art in their class for on-device use cases like\\xa0...',\n",
              "   'htmlSnippet': 'Sep 25, 2024 <b>...</b> The <b>Llama 3.2</b> 1B and 3B <b>models</b> support context length of 128K tokens and are state-of-the-art in their class for on-device use cases like&nbsp;...',\n",
              "   'formattedUrl': 'https://ai.meta.com/.../llama-3-2-connect-2024-vision-edge-mobile-devices/',\n",
              "   'htmlFormattedUrl': 'https://ai.meta.com/.../<b>llama</b>-3-2-connect-2024-vision-edge-mobile-devices/',\n",
              "   'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTK0RbHuf5EmtFc1DY85xC6R2yzKzZaJgkHU1re2O80k4uAyUHElWK3ZmP-&s',\n",
              "      'width': '300',\n",
              "      'height': '168'}],\n",
              "    'metatags': [{'og:image': 'https://scontent-atl3-2.xx.fbcdn.net/v/t39.2365-6/461179924_892945479558448_4846394290454647920_n.png?_nc_cat=105&ccb=1-7&_nc_sid=e280be&_nc_ohc=xMUluC52CLoQ7kNvgEF7mlC&_nc_zt=14&_nc_ht=scontent-atl3-2.xx&_nc_gid=AdHLnyWTdsz5PABz-c0SbMb&oh=00_AYDcsxwWcDhwhs4_cNikNTK1OdTMz3MB7EkCS-joZTmaMw&oe=67986C90',\n",
              "      'twitter:card': 'summary',\n",
              "      'twitter:title': 'Llama 3.2: Revolutionizing edge AI and vision with open, customizable models',\n",
              "      'og:site_name': 'Meta AI',\n",
              "      'og:title': 'Llama 3.2: Revolutionizing edge AI and vision with open, customizable models',\n",
              "      'bingbot': 'noarchive',\n",
              "      'title': 'Llama 3.2: Revolutionizing edge AI and vision with open, customizable models',\n",
              "      'og:description': 'Today, we’re releasing Llama 3.2, which includes small and medium-sized vision LLMs, and lightweight, text-only models that fit onto edge and mobile devices.',\n",
              "      'url': 'https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/',\n",
              "      'twitter:image': 'https://ai.meta.com/static-resource/1522018001749715/',\n",
              "      'referrer': 'origin-when-crossorigin',\n",
              "      'viewport': 'width=device-width, initial-scale=1',\n",
              "      'twitter:description': 'Today, we’re releasing Llama 3.2, which includes small and medium-sized vision LLMs, and lightweight, text-only models that fit onto edge and mobile devices.',\n",
              "      'og:url': 'https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/'}],\n",
              "    'cse_image': [{'src': 'https://ai.meta.com/static-resource/1522018001749715/'}]}},\n",
              "  {'kind': 'customsearch#result',\n",
              "   'title': 'Meta | Llama 3.2 | Kaggle',\n",
              "   'htmlTitle': 'Meta | <b>Llama 3.2</b> | Kaggle',\n",
              "   'link': 'https://www.kaggle.com/models/metaresearch/llama-3.2',\n",
              "   'displayLink': 'www.kaggle.com',\n",
              "   'snippet': 'Sep 25, 2024 ... Model Architecture: Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use\\xa0...',\n",
              "   'htmlSnippet': 'Sep 25, 2024 <b>...</b> <b>Model</b> Architecture: <b>Llama 3.2</b> is an auto-regressive language <b>model</b> that uses an optimized transformer architecture. The tuned versions use&nbsp;...',\n",
              "   'formattedUrl': 'https://www.kaggle.com/models/metaresearch/llama-3.2',\n",
              "   'htmlFormattedUrl': 'https://www.kaggle.com/<b>model</b>s/metaresearch/<b>llama</b>-<b>3.2</b>',\n",
              "   'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQoAPBWzD8DbMu8jvxLDvF-o71CpsZ2pQYORxRRVU22zryCHSN4C--cV8UG&s',\n",
              "      'width': '225',\n",
              "      'height': '225'}],\n",
              "    'metatags': [{'theme-color': '#008ABC',\n",
              "      'og:type': 'website',\n",
              "      'fb:app_id': '2665027677054710',\n",
              "      'twitter:card': 'summary',\n",
              "      'twitter:site': '@Kaggle',\n",
              "      'viewport': 'width=device-width, initial-scale=1.0, maximum-scale=5.0, minimum-scale=1.0',\n",
              "      'og:title': 'Meta | Llama 3.2 | Kaggle',\n",
              "      'og:url': 'https://www.kaggle.com/models/metaresearch/llama-3.2',\n",
              "      'og:description': 'The Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out).'}],\n",
              "    'cse_image': [{'src': 'https://storage.googleapis.com/kaggle-organizations/3837/thumbnail-2.png'}]}},\n",
              "  {'kind': 'customsearch#result',\n",
              "   'title': 'Llama',\n",
              "   'htmlTitle': '<b>Llama</b>',\n",
              "   'link': 'https://www.llama.com/',\n",
              "   'displayLink': 'www.llama.com',\n",
              "   'snippet': 'The open-source AI models you can fine-tune, distill and deploy anywhere. Choose from our collection of models: Llama 3.1, Llama 3.2, Llama 3.3.',\n",
              "   'htmlSnippet': 'The open-source AI <b>models</b> you can fine-tune, distill and deploy anywhere. Choose from our collection of <b>models</b>: <b>Llama</b> 3.1, <b>Llama 3.2</b>, <b>Llama</b> 3.3.',\n",
              "   'formattedUrl': 'https://www.llama.com/',\n",
              "   'htmlFormattedUrl': 'https://www.<b>llama</b>.com/',\n",
              "   'pagemap': {'metatags': [{'og:image': 'https://scontent-lax3-1.xx.fbcdn.net/v/t39.2365-6/469434740_940886534071882_1120629007224700925_n.jpg?_nc_cat=109&ccb=1-7&_nc_sid=aa6a2f&_nc_ohc=qgphdPq-sC8Q7kNvgHIr1b4&_nc_zt=14&_nc_ht=scontent-lax3-1.xx&_nc_gid=AAQHUOzl2OIX7mONjz0mhin&oh=00_AYAFrV_X0UP7wYtZi5UL9fIFwDBjXB0SkajKMQd0skeIvg&oe=67841BD5',\n",
              "      'twitter:card': 'summary',\n",
              "      'twitter:title': 'Llama',\n",
              "      'og:site_name': 'Meta Llama',\n",
              "      'viewport': 'width=device-width,initial-scale=1,maximum-scale=2,shrink-to-fit=no',\n",
              "      'twitter:description': 'The open-source AI models you can fine-tune, distill and deploy anywhere. Choose from our collection of models: Llama 3.1, Llama 3.2, Llama 3.3.',\n",
              "      'og:title': 'Llama',\n",
              "      'bingbot': 'noarchive',\n",
              "      'title': 'Llama',\n",
              "      'og:description': 'The open-source AI models you can fine-tune, distill and deploy anywhere. Choose from our collection of models: Llama 3.1, Llama 3.2, Llama 3.3.',\n",
              "      'twitter:image': 'https://www.llama.com/static-resource/2041362162899740/'}],\n",
              "    'cse_image': [{'src': 'https://scontent-lax3-1.xx.fbcdn.net/v/t39.2365-6/469434740_940886534071882_1120629007224700925_n.jpg?_nc_cat=109&ccb=1-7&_nc_sid=aa6a2f&_nc_ohc=qgphdPq-sC8Q7kNvgHIr1b4&_nc_zt=14&_nc_ht=scontent-lax3-1.xx&_nc_gid=AAQHUOzl2OIX7mONjz0mhin&oh=00_AYAFrV_X0UP7wYtZi5UL9fIFwDBjXB0SkajKMQd0skeIvg&oe=67841BD5'}]}},\n",
              "  {'kind': 'customsearch#result',\n",
              "   'title': 'Llama 3.2 models from Meta are now available in Amazon ...',\n",
              "   'htmlTitle': '<b>Llama 3.2 models</b> from Meta are now available in Amazon ...',\n",
              "   'link': 'https://aws.amazon.com/blogs/machine-learning/llama-3-2-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/',\n",
              "   'displayLink': 'aws.amazon.com',\n",
              "   'snippet': 'Sep 25, 2024 ... Llama 3.2 models are offered in various sizes, from small and medium-sized multi-modal models. The larger Llama 3.2 models come in two parameter\\xa0...',\n",
              "   'htmlSnippet': 'Sep 25, 2024 <b>...</b> <b>Llama 3.2 models</b> are offered in various sizes, from small and medium-sized multi-modal <b>models</b>. The larger <b>Llama 3.2 models</b> come in two parameter&nbsp;...',\n",
              "   'formattedUrl': 'https://aws.amazon.com/.../llama-3-2-models-from-meta-are-now-available-...',\n",
              "   'htmlFormattedUrl': 'https://aws.amazon.com/.../<b>llama</b>-3-2-<b>model</b>s-from-meta-are-now-available-...',\n",
              "   'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTDrXZSZKQPyymkJMwsG-DbmUquTsEXbhJFxwAm9Rvd67zVAkAP9p5XP7QD&s',\n",
              "      'width': '300',\n",
              "      'height': '168'}],\n",
              "    'TechArticle': [{'datePublished': '2024-09-25T10:26:23-08:00',\n",
              "      'noopener': 'Amazon SageMaker JumpStart',\n",
              "      'image': 'https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/09/25/llama3.2-blog-entry-1120x630.png',\n",
              "      'articleBody': 'Today, we are excited to announce the availability of Llama 3.2 models in Amazon SageMaker JumpStart. Llama 3.2 oﬀers multi-modal vision and lightweight models representing Meta’s latest...',\n",
              "      'name': 'Llama 3.2 models from Meta are now available in Amazon SageMaker JumpStart',\n",
              "      'inLanguage': 'en-US',\n",
              "      'headline': 'Llama 3.2 models from Meta are now available in Amazon SageMaker JumpStart',\n",
              "      'articleSection': 'Amazon SageMaker JumpStart',\n",
              "      'url': 'https://aws.amazon.com/blogs/machine-learning/llama-3-2-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/'}],\n",
              "    'metatags': [{'og:image': 'https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/09/25/llama3.2-blog-entry-1120x630.png',\n",
              "      'og:image:width': '1120',\n",
              "      'article:published_time': '2024-09-25T10:26:23-08:00',\n",
              "      'twitter:card': 'summary_large_image',\n",
              "      'og:site_name': 'Amazon Web Services',\n",
              "      'og:description': 'In this post, we show how you can discover and deploy the Llama 3.2 11B Vision model using SageMaker JumpStart. We also share the supported instance types and context for all the Llama 3.2 models available in SageMaker JumpStart.',\n",
              "      'article:publisher': 'https://www.facebook.com/amazonwebservices',\n",
              "      'twitter:image': 'https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/09/25/llama3.2-blog-entry-1120x630.png',\n",
              "      'bp-token-api': 'https://auth.aws.amazon.com',\n",
              "      'twitter:site': '@awscloud',\n",
              "      'article:modified_time': '2024-09-30T12:10:59-08:00',\n",
              "      'image': 'https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/09/25/llama3.2-blog-entry-1120x630.png',\n",
              "      'og:type': 'article',\n",
              "      'article:section': 'Amazon SageMaker JumpStart',\n",
              "      'twitter:title': 'Llama 3.2 models from Meta are now available in Amazon SageMaker JumpStart | Amazon Web Services',\n",
              "      'twitter:domain': 'https://aws.amazon.com/blogs/',\n",
              "      'inlanguage': 'en-US',\n",
              "      'og:title': 'Llama 3.2 models from Meta are now available in Amazon SageMaker JumpStart | Amazon Web Services',\n",
              "      'og:image:height': '630',\n",
              "      'og:updated_time': '2024-09-30T12:10:59-08:00',\n",
              "      'article:tag': 'Amazon SageMaker JumpStart',\n",
              "      'bp-app-id': 'awt-prod',\n",
              "      'viewport': 'width=device-width, initial-scale=1',\n",
              "      'twitter:description': 'In this post, we show how you can discover and deploy the Llama 3.2 11B Vision model using SageMaker JumpStart. We also share the supported instance types and context for all the Llama 3.2 models available in SageMaker JumpStart.',\n",
              "      'og:locale': 'en_US',\n",
              "      'bp-token-cookie-key': 'awsm-session',\n",
              "      'og:url': 'https://aws.amazon.com/blogs/machine-learning/llama-3-2-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/'}],\n",
              "    'cse_image': [{'src': 'https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2024/09/25/llama3.2-blog-entry-1120x630.png'}]}},\n",
              "  {'kind': 'customsearch#result',\n",
              "   'title': 'Model Cards and Prompt formats - Llama 3.2',\n",
              "   'htmlTitle': 'Model Cards and Prompt formats - Llama 3.2',\n",
              "   'link': 'https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2/',\n",
              "   'displayLink': 'www.llama.com',\n",
              "   'snippet': 'Prompts designed with the non-quantized models will work without modification on the quantized models. For how to design prompts to access the features of the\\xa0...',\n",
              "   'htmlSnippet': 'Prompts designed with the non-quantized <b>models</b> will work without modification on the quantized <b>models</b>. For how to design prompts to access the features of the&nbsp;...',\n",
              "   'formattedUrl': 'https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2/',\n",
              "   'htmlFormattedUrl': 'https://www.<b>llama</b>.com/docs/<b>model</b>-cards-and-prompt-formats/<b>llama</b>3_2/',\n",
              "   'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQOuedp1JlZonGr_YOpy3u1kPboZH5PKGkAZ42c_IqFce0ZHlAa25SD6LtY&s',\n",
              "      'width': '310',\n",
              "      'height': '163'}],\n",
              "    'metatags': [{'og:image': 'GKtafBvy07VJ5DkEAG6Ing2qnAJsbj0JAABZ',\n",
              "      'twitter:card': 'summary',\n",
              "      'twitter:title': 'Llama 3.2 | Model Cards and Prompt formats',\n",
              "      'viewport': 'width=device-width,initial-scale=1,maximum-scale=2,shrink-to-fit=no',\n",
              "      'twitter:description': '.',\n",
              "      'og:title': 'Llama 3.2 | Model Cards and Prompt formats',\n",
              "      'bingbot': 'noarchive',\n",
              "      'title': 'Llama 3.2 | Model Cards and Prompt formats',\n",
              "      'og:description': '.',\n",
              "      'twitter:image': 'https://www.llama.com/static-resource/903729188343068/'}],\n",
              "    'cse_image': [{'src': 'https://www.llama.com/static-resource/903729188343068/'}]}},\n",
              "  {'kind': 'customsearch#result',\n",
              "   'title': 'Introducing Multimodal Llama 3.2 - DeepLearning.AI',\n",
              "   'htmlTitle': 'Introducing Multimodal <b>Llama 3.2</b> - DeepLearning.AI',\n",
              "   'link': 'https://www.deeplearning.ai/short-courses/introducing-multimodal-llama-3-2/',\n",
              "   'displayLink': 'www.deeplearning.ai',\n",
              "   'snippet': 'Try out the features of the new Llama 3.2 models to build AI applications with multimodality.',\n",
              "   'htmlSnippet': 'Try out the features of the new <b>Llama 3.2 models</b> to build AI applications with multimodality.',\n",
              "   'formattedUrl': 'https://www.deeplearning.ai/short.../introducing-multimodal-llama-3-2/',\n",
              "   'htmlFormattedUrl': 'https://www.deeplearning.ai/short.../introducing-multimodal-<b>llama</b>-3-2/',\n",
              "   'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTKvYGDWTX7WZZuWhzRVAzd0Rq60PGnA0QpJ1KBiiPFVC-jefw-7Ygdkc8&s',\n",
              "      'width': '311',\n",
              "      'height': '162'}],\n",
              "    'metatags': [{'og:image': 'https://home-wordpress.deeplearning.ai/wp-content/uploads/2024/09/V6_DeepLearning_Meta_Multimodal_Llama_Banner_2070x1080.png',\n",
              "      'og:type': 'website',\n",
              "      'og:image:width': '4291',\n",
              "      'twitter:card': 'summary_large_image',\n",
              "      'twitter:title': 'Introducing Multimodal Llama 3.2 - DeepLearning.AI',\n",
              "      'og:title': 'Introducing Multimodal Llama 3.2',\n",
              "      'og:image:height': '2234',\n",
              "      'og:description': 'Try out the features of the new Llama 3.2 models to build AI applications with multimodality.',\n",
              "      'twitter:creator': '@deeplearningai_',\n",
              "      'next-head-count': '19',\n",
              "      'twitter:site': '@deeplearningai_',\n",
              "      'viewport': 'width=device-width',\n",
              "      'twitter:description': 'Try out the features of the new Llama 3.2 models to build AI applications with multimodality.',\n",
              "      'og:locale': 'en_US',\n",
              "      'og:url': 'https://www.deeplearning.ai/short-courses/introducing-multimodal-llama-3-2/'}],\n",
              "    'cse_image': [{'src': 'https://home-wordpress.deeplearning.ai/wp-content/uploads/2024/09/V6_DeepLearning_Meta_Multimodal_Llama_Banner_2070x1080.png'}]}},\n",
              "  {'kind': 'customsearch#result',\n",
              "   'title': 'Llama 3.2: Advanced Vision and Edge AI Models for Mobile and ...',\n",
              "   'htmlTitle': '<b>Llama 3.2</b>: Advanced Vision and Edge AI <b>Models</b> for Mobile and ...',\n",
              "   'link': 'https://encord.com/blog/lama-3-2-explained/',\n",
              "   'displayLink': 'encord.com',\n",
              "   'snippet': 'Sep 30, 2024 ... Key Features of Llama 3.2. Expanded Model Variants: Llama 3.2 offers both lightweight large language models (LLMs) (1B and 3B) and medium\\xa0...',\n",
              "   'htmlSnippet': 'Sep 30, 2024 <b>...</b> Key Features of <b>Llama 3.2</b>. Expanded <b>Model</b> Variants: <b>Llama 3.2</b> offers both lightweight large language <b>models</b> (LLMs) (1B and 3B) and medium&nbsp;...',\n",
              "   'formattedUrl': 'https://encord.com/blog/lama-3-2-explained/',\n",
              "   'htmlFormattedUrl': 'https://encord.com/blog/lama-3-2-explained/',\n",
              "   'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRbHvq2FEHDjemjgXqEU977wRFiLDg0Fg_s0hMt4wOlO_vaCmKKu2XdGvL3&s',\n",
              "      'width': '300',\n",
              "      'height': '168'}],\n",
              "    'metatags': [{'og:image': 'https://images.prismic.io/encord/ZvrmNrVsGrYSwK0B_image-148-.png?auto=format%2Ccompress&fit=max',\n",
              "      'og:type': 'article',\n",
              "      'og:image:width': '2167',\n",
              "      'twitter:card': 'summary_large_image',\n",
              "      'twitter:title': 'From Vision to Edge: Meta’s Llama 3.2 Explained',\n",
              "      'og:title': 'From Vision to Edge: Meta’s Llama 3.2 Explained',\n",
              "      'og:image:height': '1078',\n",
              "      'og:image:type': 'image/jpeg',\n",
              "      'og:description': 'Meta just released Llama 3.2, the next era of its open sourced AI models building on Llama 3.1. It introduces high-performance lightweight model',\n",
              "      'twitter:creator': '@encord_team',\n",
              "      'twitter:image': 'https://images.prismic.io/encord/ZvrmNrVsGrYSwK0B_image-148-.png?auto=format%2Ccompress&fit=max',\n",
              "      'twitter:site': '@encord_team',\n",
              "      'viewport': 'width=device-width, initial-scale=1, shrink-to-fit=no',\n",
              "      'twitter:description': 'Meta just released Llama 3.2, the next era of its open sourced AI models building on Llama 3.1. It introduces high-performance lightweight model',\n",
              "      'og:url': 'https://encord.com/blog/lama-3-2-explained/'}],\n",
              "    'cse_image': [{'src': 'https://images.prismic.io/encord/ZvrmNrVsGrYSwK0B_image-148-.png?auto=format%2Ccompress&fit=max'}]}},\n",
              "  {'kind': 'customsearch#result',\n",
              "   'title': 'Llama 3.2 Requirements [What you Need to Use It?]',\n",
              "   'htmlTitle': '<b>Llama 3.2</b> Requirements [What you Need to Use It?]',\n",
              "   'link': 'https://llamaimodel.com/requirements-3-2/',\n",
              "   'displayLink': 'llamaimodel.com',\n",
              "   'snippet': 'Llama 3.2 1B Instruct Model Specifications · Multilingual Support, 8 languages: English, German, French, Italian, Portuguese, Hindi, Spanish, Thai ; Hardware\\xa0...',\n",
              "   'htmlSnippet': '<b>Llama 3.2</b> 1B Instruct <b>Model Specifications</b> &middot; Multilingual Support, 8 languages: English, German, French, Italian, Portuguese, Hindi, Spanish, Thai ; Hardware&nbsp;...',\n",
              "   'formattedUrl': 'https://llamaimodel.com/requirements-3-2/',\n",
              "   'htmlFormattedUrl': 'https://<b>llama</b>i<b>model</b>.com/requirements-3-2/',\n",
              "   'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcStxOv0IOg8Jas12p2BMNSZo1hLG7pODNzdEJYoRrUvJODulp_JoWDDV2M8&s',\n",
              "      'width': '306',\n",
              "      'height': '165'}],\n",
              "    'metatags': [{'og:image': 'https://llamaimodel.com/wp-content/uploads/2024/04/Llama-3-Requirements_.webp',\n",
              "      'og:type': 'article',\n",
              "      'og:image:width': '600',\n",
              "      'twitter:card': 'summary_large_image',\n",
              "      'og:site_name': 'Llama Ai Model',\n",
              "      'og:title': 'Llama 3.2 Requirements [What you Need to Use It?] 💻',\n",
              "      'og:image:height': '324',\n",
              "      'twitter:label1': 'Est. reading time',\n",
              "      'og:image:type': 'image/webp',\n",
              "      'msapplication-tileimage': 'https://llamaimodel.com/wp-content/uploads/2024/04/cropped-Llama-AI-3-270x270.webp',\n",
              "      'og:description': 'Llama 3.2 represents a significant advancement in the field of AI language models. With variants ranging from 1B to 90B parameters, this series offers solutions for a wide array of applications, from edge devices to large-scale cloud deployments. Llama 3.2 1B Instruct Requirements Category Requirement Details Llama 3.2 1B Instruct Model Specifications Parameters 1 billion ... Read more',\n",
              "      'twitter:data1': '6 minutes',\n",
              "      'article:modified_time': '2024-12-11T07:58:59+00:00',\n",
              "      'viewport': 'width=device-width, initial-scale=1',\n",
              "      'og:locale': 'en_US',\n",
              "      'og:url': 'https://llamaimodel.com/requirements-3-2/'}],\n",
              "    'creativework': [{'text': 'Llama 3.2 represents a significant advancement in the field of AI language models. With variants ranging from 1B to 90B parameters, this series offers solutions for a wide array of applications,...',\n",
              "      'headline': 'Llama 3.2 Requirements'}],\n",
              "    'cse_image': [{'src': 'https://llamaimodel.com/wp-content/uploads/2024/04/Llama-3-Requirements_.webp'}]}},\n",
              "  {'kind': 'customsearch#result',\n",
              "   'title': 'Introducing Multimodal Llama 3.2',\n",
              "   'htmlTitle': 'Introducing Multimodal <b>Llama 3.2</b>',\n",
              "   'link': 'https://www.coursera.org/projects/introducing-multimodal-llama-3-2',\n",
              "   'displayLink': 'www.coursera.org',\n",
              "   'snippet': 'Explore the features of the new Llama 3.2 model, from image classification, vision reasoning to tool use. Learn the details of Llama 3.2 prompting.',\n",
              "   'htmlSnippet': 'Explore the features of the new <b>Llama 3.2 model</b>, from image classification, vision reasoning to tool use. Learn the <b>details</b> of <b>Llama 3.2</b> prompting.',\n",
              "   'formattedUrl': 'https://www.coursera.org/projects/introducing-multimodal-llama-3-2',\n",
              "   'htmlFormattedUrl': 'https://www.coursera.org/projects/introducing-multimodal-<b>llama</b>-3-2',\n",
              "   'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRLOrzGFWIhtl3FZ-A9j9OsEB433TjypN9kmhTPw66KOqB5O36T6eqL3VHh&s',\n",
              "      'width': '310',\n",
              "      'height': '162'}],\n",
              "    'metatags': [{'twitter:app:id:googleplay': 'org.coursera.android',\n",
              "      'qc:admins': '366737676376375235216727',\n",
              "      'og:image': 'https://s3.amazonaws.com/coursera_assets/meta_images/generated/XDP/XDP~COURSE!~introducing-multimodal-llama-3-2/XDP~COURSE!~introducing-multimodal-llama-3-2.jpeg',\n",
              "      'theme-color': '#0056D2',\n",
              "      'twitter:card': 'summary',\n",
              "      'og:site_name': 'Coursera',\n",
              "      'twitter:app:name:googleplay': 'Coursera',\n",
              "      'twitter:app:id:iphone': 'id736535961',\n",
              "      'msapplication-tileimage': 'https://d3njjcbhbojbot.cloudfront.net/web/images/favicons/mstile-v2-144x144.png',\n",
              "      'og:description': 'Complete this Guided Project in under 2 hours. Join our new short course, Introducing Multimodal Llama 3.2, and learn from Amit Sangani, Senior Director of ...',\n",
              "      'twitter:image': 'https://s3.amazonaws.com/coursera_assets/meta_images/generated/XDP/XDP~COURSE!~introducing-multimodal-llama-3-2/XDP~COURSE!~introducing-multimodal-llama-3-2.jpeg',\n",
              "      'twitter:site': 'Coursera',\n",
              "      'fb:admins': '727836538,4807654',\n",
              "      'msapplication-tilecolor': '#2d89ef',\n",
              "      'image': 'https://s3.amazonaws.com/coursera_assets/meta_images/generated/XDP/XDP~COURSE!~introducing-multimodal-llama-3-2/XDP~COURSE!~introducing-multimodal-llama-3-2.jpeg',\n",
              "      'og:type': 'website',\n",
              "      'twitter:title': 'Introducing Multimodal Llama 3.2',\n",
              "      'og:title': 'Introducing Multimodal Llama 3.2',\n",
              "      'twitter:app:id:ipad': 'id736535961',\n",
              "      'yandex-verification': '4970cfdb825622c7',\n",
              "      'twitter:image:src': 'https://s3.amazonaws.com/coursera_assets/meta_images/generated/XDP/XDP~COURSE!~introducing-multimodal-llama-3-2/XDP~COURSE!~introducing-multimodal-llama-3-2.jpeg',\n",
              "      'fb:app_id': '823425307723964',\n",
              "      'twitter:app:name:ipad': 'Coursera',\n",
              "      'viewport': 'width=device-width, initial-scale=1',\n",
              "      'twitter:description': 'Complete this Guided Project in under 2 hours. Join our ...',\n",
              "      'og:locale': 'en_US',\n",
              "      'og:url': 'https://www.coursera.org/projects/introducing-multimodal-llama-3-2',\n",
              "      'twitter:app:name:iphone': 'Coursera'}],\n",
              "    'cse_image': [{'src': 'https://s3.amazonaws.com/coursera_assets/meta_images/generated/XDP/XDP~COURSE!~introducing-multimodal-llama-3-2/XDP~COURSE!~introducing-multimodal-llama-3-2.jpeg'}]}},\n",
              "  {'kind': 'customsearch#result',\n",
              "   'title': 'Meta Llama 3.2 3B Instruct ONNX INT4 RTX | NVIDIA NGC',\n",
              "   'htmlTitle': 'Meta <b>Llama 3.2</b> 3B Instruct ONNX INT4 RTX | NVIDIA NGC',\n",
              "   'link': 'https://catalog.ngc.nvidia.com/orgs/nvidia/models/meta-llama-3.2-3b-onnx-int4-rtx',\n",
              "   'displayLink': 'catalog.ngc.nvidia.com',\n",
              "   'snippet': 'Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and\\xa0...',\n",
              "   'htmlSnippet': '<b>Llama 3.2</b> is an auto-regressive language <b>model</b> that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and&nbsp;...',\n",
              "   'formattedUrl': 'https://catalog.ngc.nvidia.com/orgs/.../models/meta-llama-3.2-3b-onnx-int4...',\n",
              "   'htmlFormattedUrl': 'https://catalog.ngc.nvidia.com/orgs/.../<b>model</b>s/meta-<b>llama</b>-<b>3.2</b>-3b-onnx-int4...',\n",
              "   'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQRslUzHYY014KHVoNw-yfadqsIkuK9NHCYMqzz-buOJFjTdFZGJm2dauI&s',\n",
              "      'width': '321',\n",
              "      'height': '157'}],\n",
              "    'metatags': [{'next-head-count': '9',\n",
              "      'og:type': 'website',\n",
              "      'og:site_name': 'NVIDIA NGC Catalog',\n",
              "      'viewport': 'width=device-width',\n",
              "      'og:title': 'Meta Llama 3.2 3B Instruct ONNX INT4 RTX | NVIDIA NGC',\n",
              "      'og:description': 'Built with Llama - The Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned text-to-text generative models in 1B and 3B sizes.'}],\n",
              "    'cse_image': [{'src': 'https://catalog.ngc.nvidia.com/_next/image?url=https%3A%2F%2Fcatalog.ngc.nvidia.com%2Fdemos%2Fllama2.jpg&w=828&q=90'}]}}]}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHALd3uhIxtQ"
      },
      "source": [
        "## Read Each URL Contents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXz3JFduBsaq",
        "outputId": "44ffb45d-ed0e-4512-a89e-7fd92b626a64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "import newspaper\n",
        "\n",
        "pages_content = []\n",
        "\n",
        "for item in search_results[\"items\"]:\n",
        "\n",
        "    try:\n",
        "        article = newspaper.Article(item[\"link\"])\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        if len(article.text) > 0:\n",
        "            pages_content.append(\n",
        "                {\"url\": item[\"link\"], \"text\": article.text, \"title\": item[\"title\"]}\n",
        "            )\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "print(len(pages_content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqxa_qRVI3G0"
      },
      "source": [
        "## Create the Index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "O4PkK8DuBsZT"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Document\n",
        "\n",
        "# Convert the texts to Document objects so the LlamaIndex framework can process them.\n",
        "documents = [\n",
        "    Document(text=row[\"text\"], metadata={\"title\": row[\"title\"], \"url\": row[\"url\"]})\n",
        "    for row in pages_content\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "2RtMBWpgBsWX"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "# Build index / generate embeddings using OpenAI.\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    transformations=[SentenceSplitter(chunk_size=512, chunk_overlap=128)],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xV_ibEZ_BsM4"
      },
      "outputs": [],
      "source": [
        "# Define a query engine that is responsible for retrieving related pieces of text,\n",
        "# and using a LLM to formulate the final answer.\n",
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nziwu27MI6ih"
      },
      "source": [
        "## Query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xea7ZeidH27i",
        "outputId": "0d3d8fd1-e6fc-49ce-a369-f8c272159c4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The LLaMA 3.2 model has variants with the following exact sizes in terms of parameters:\n",
            "\n",
            "- 1B (1 billion parameters)\n",
            "- 3B (3 billion parameters)\n",
            "- 90B (90 billion parameters)\n"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(\"How many parameters LLaMA 3.2 model has? list exact sizes of the models\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QpGPD5nHORP",
        "outputId": "23885de6-7c2a-4606-e0c1-52d8fa5ac993"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title\t Llama 3.2 Requirements [What you Need to Use It?]\n",
            "Source\t https://llamaimodel.com/requirements-3-2/\n",
            "Score\t 0.5604090406471472\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Title\t Llama 3.2 Requirements [What you Need to Use It?]\n",
            "Source\t https://llamaimodel.com/requirements-3-2/\n",
            "Score\t 0.5404580362425195\n",
            "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "# Show the retrieved nodes\n",
        "for src in response.source_nodes:\n",
        "    print(\"Title\\t\", src.metadata[\"title\"])\n",
        "    print(\"Source\\t\", src.metadata[\"url\"])\n",
        "    print(\"Score\\t\", src.score)\n",
        "    print(\"-_\" * 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5b4nZ-qHpdP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}